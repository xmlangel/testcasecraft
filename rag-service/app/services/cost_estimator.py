"""Cost Estimator Service for LLM API usage"""
from typing import Dict, List
from sqlalchemy.orm import Session
from ..models.rag_embedding import RAGEmbedding
from ..schemas.llm_analysis import CostEstimateResponse, CostBreakdown, ModelPricing
from uuid import UUID


class CostEstimator:
    """LLM API ë¹„ìš© ì¶”ì • ì„œë¹„ìŠ¤"""

    # LLM ê°€ê²©í‘œ (USD per 1K tokens) - 2025ë…„ 1ì›” ê¸°ì¤€
    PRICING = {
        "openai": {
            "gpt-4": {"input": 0.01, "output": 0.06},
            "gpt-4-turbo": {"input": 0.01, "output": 0.03},
            "gpt-4-turbo-preview": {"input": 0.01, "output": 0.03},
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
            "gpt-3.5-turbo-16k": {"input": 0.001, "output": 0.002},
        },
        "anthropic": {
            "claude-3-opus": {"input": 0.015, "output": 0.075},
            "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
            "claude-3-sonnet": {"input": 0.003, "output": 0.015},
            "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
            "claude-3-haiku": {"input": 0.00025, "output": 0.00125},
            "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        },
        "ollama": {
            # ë¡œì»¬ ëª¨ë¸ì€ ë¹„ìš© ì—†ìŒ
            "*": {"input": 0, "output": 0}
        }
    }

    def __init__(self, db: Session):
        self.db = db

    def estimate_analysis_cost(
        self,
        document_id: UUID,
        llm_provider: str,
        llm_model: str,
        prompt_template: str,
        max_tokens: int
    ) -> CostEstimateResponse:
        """ë¶„ì„ ë¹„ìš© ì¶”ì •

        Args:
            document_id: ë¬¸ì„œ ID
            llm_provider: LLM ì œê³µì (openai, anthropic, ollama, openwebui, openrouter, perplexity ë“±)
            llm_model: LLM ëª¨ë¸ëª…
            prompt_template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ({chunk_text} í¬í•¨)
            max_tokens: ìµœëŒ€ ì‘ë‹µ í† í° ìˆ˜

        Returns:
            CostEstimateResponse: ë¹„ìš© ì¶”ì • ê²°ê³¼
        """

        # 1. ì²­í¬ ì¡°íšŒ
        chunks = self.db.query(RAGEmbedding)\
            .filter(RAGEmbedding.document_id == document_id)\
            .all()

        total_chunks = len(chunks)

        if total_chunks == 0:
            # ì²­í¬ê°€ ì—†ëŠ” ê²½ìš°
            return self._create_empty_response(document_id, llm_provider, llm_model)

        # 2. í‰ê·  ì²­í¬ í¬ê¸° ê³„ì‚° (í† í° ì¶”ì •)
        total_chunk_length = sum(len(chunk.chunk_text) for chunk in chunks)
        avg_chunk_length = total_chunk_length / total_chunks

        # í† í° ì¶”ì •: 1 token â‰ˆ 4 characters (ì˜ì–´ ê¸°ì¤€)
        # í•œê¸€ì€ 1 token â‰ˆ 1-2 characters ì •ë„
        # ë³´ìˆ˜ì ìœ¼ë¡œ 2.5 characters per token ì‚¬ìš©
        avg_chunk_tokens = int(avg_chunk_length / 2.5)

        # 3. í”„ë¡¬í”„íŠ¸ ì˜¤ë²„í—¤ë“œ ì¶”ì •
        prompt_overhead_text = prompt_template.replace("{chunk_text}", "")
        prompt_overhead_tokens = int(len(prompt_overhead_text) / 2.5)

        # 4. ì´ ì…ë ¥/ì¶œë ¥ í† í° ì¶”ì •
        estimated_input_tokens_per_chunk = avg_chunk_tokens + prompt_overhead_tokens
        estimated_output_tokens_per_chunk = max_tokens

        total_input_tokens = estimated_input_tokens_per_chunk * total_chunks
        total_output_tokens = estimated_output_tokens_per_chunk * total_chunks
        total_tokens = total_input_tokens + total_output_tokens

        # 5. ë¹„ìš© ê³„ì‚°
        pricing = self._get_pricing(llm_provider, llm_model)

        input_cost = (total_input_tokens / 1000) * pricing["input"]
        output_cost = (total_output_tokens / 1000) * pricing["output"]
        total_cost = input_cost + output_cost

        cost_per_chunk = total_cost / total_chunks if total_chunks > 0 else 0

        # 6. ê²½ê³  ë©”ì‹œì§€ ìƒì„±
        warnings = self._generate_warnings(
            total_cost, total_chunks, llm_provider, avg_chunk_length
        )

        # 7. ì‘ë‹µ ìƒì„±
        return CostEstimateResponse(
            document_id=document_id,
            total_chunks=total_chunks,
            estimated_input_tokens=total_input_tokens,
            estimated_output_tokens=total_output_tokens,
            estimated_total_tokens=total_tokens,
            cost_breakdown=CostBreakdown(
                input_cost_usd=round(input_cost, 4),
                output_cost_usd=round(output_cost, 4),
                total_cost_usd=round(total_cost, 4)
            ),
            cost_per_chunk_usd=round(cost_per_chunk, 4),
            model_pricing=ModelPricing(
                provider=llm_provider,
                model=llm_model,
                input_price_per_1k=pricing["input"],
                output_price_per_1k=pricing["output"]
            ),
            warnings=warnings
        )

    def _get_pricing(self, provider: str, model: str) -> Dict[str, float]:
        """ëª¨ë¸ ê°€ê²© ì¡°íšŒ

        Args:
            provider: LLM ì œê³µì
            model: ëª¨ë¸ëª…

        Returns:
            Dict[str, float]: {"input": ì…ë ¥ ê°€ê²©, "output": ì¶œë ¥ ê°€ê²©}
        """
        provider_lower = provider.lower()
        model_lower = model.lower()

        if provider_lower not in self.PRICING:
            # ì•Œ ìˆ˜ ì—†ëŠ” ì œê³µì: ê¸°ë³¸ê°’ ë°˜í™˜
            return {"input": 0.01, "output": 0.03}

        provider_pricing = self.PRICING[provider_lower]

        # ì •í™•í•œ ëª¨ë¸ëª… ë§¤ì¹­
        if model_lower in provider_pricing:
            return provider_pricing[model_lower]

        # ì™€ì¼ë“œì¹´ë“œ ë§¤ì¹­ (Ollama ë“±)
        if "*" in provider_pricing:
            return provider_pricing["*"]

        # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (gpt-4-1106-preview â†’ gpt-4)
        for key in provider_pricing.keys():
            if model_lower.startswith(key):
                return provider_pricing[key]

        # ê¸°ë³¸ê°’ ë°˜í™˜
        return {"input": 0.01, "output": 0.03}

    def _generate_warnings(
        self,
        total_cost: float,
        total_chunks: int,
        provider: str,
        avg_chunk_length: float
    ) -> List[str]:
        """ê²½ê³  ë©”ì‹œì§€ ìƒì„±

        Args:
            total_cost: ì´ ì˜ˆìƒ ë¹„ìš© (USD)
            total_chunks: ì „ì²´ ì²­í¬ ìˆ˜
            provider: LLM ì œê³µì
            avg_chunk_length: í‰ê·  ì²­í¬ ê¸¸ì´

        Returns:
            List[str]: ê²½ê³  ë©”ì‹œì§€ ëª©ë¡
        """
        warnings = []

        # ë¹„ìš© ê²½ê³ 
        if provider.lower() == "ollama":
            warnings.append("âœ… ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë¯€ë¡œ API ë¹„ìš©ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            if total_cost > 20:
                warnings.append(
                    f"âš ï¸ ë§¤ìš° ë†’ì€ ë¹„ìš© ê²½ê³ : ì´ ì‘ì—…ì€ ì•½ ${total_cost:.2f}ì˜ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
            elif total_cost > 10:
                warnings.append(
                    f"âš ï¸ ë†’ì€ ë¹„ìš© ê²½ê³ : ì´ ì‘ì—…ì€ ì•½ ${total_cost:.2f}ì˜ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
            elif total_cost > 1:
                warnings.append(
                    f"ğŸ’µ ì´ ì‘ì—…ì€ ì•½ ${total_cost:.2f}ì˜ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
            else:
                warnings.append(f"ğŸ’µ ì˜ˆìƒ ë¹„ìš©: ì•½ ${total_cost:.2f}")

        # ì²­í¬ ì •ë³´
        warnings.append(f"ğŸ“„ ì´ {total_chunks}ê°œ ì²­í¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # ì²­í¬ í¬ê¸° ê²½ê³ 
        if avg_chunk_length > 3000:
            warnings.append(
                f"âš ï¸ ì²­í¬ í¬ê¸°ê°€ í½ë‹ˆë‹¤ (í‰ê·  {int(avg_chunk_length)}ì). "
                "ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ì´ ì¶”ì •ë³´ë‹¤ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        # ì¼ë°˜ ì£¼ì˜ì‚¬í•­
        warnings.append(
            "â„¹ï¸ ì‹¤ì œ ë¹„ìš©ì€ ì²­í¬ í¬ê¸°, LLM ì‘ë‹µ ê¸¸ì´, API ê°€ê²© ë³€ë™ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        warnings.append(
            "ğŸ’¡ ë°°ì¹˜ í™•ì¸ ê¸°ëŠ¥(10ê°œ ë‹¨ìœ„)ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ê°„ì— ë¹„ìš©ì„ í™•ì¸í•˜ê³  ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

        return warnings

    def _create_empty_response(
        self,
        document_id: UUID,
        llm_provider: str,
        llm_model: str
    ) -> CostEstimateResponse:
        """ì²­í¬ê°€ ì—†ëŠ” ê²½ìš°ì˜ ë¹ˆ ì‘ë‹µ ìƒì„±

        Args:
            document_id: ë¬¸ì„œ ID
            llm_provider: LLM ì œê³µì
            llm_model: ëª¨ë¸ëª…

        Returns:
            CostEstimateResponse: ë¹ˆ ë¹„ìš© ì¶”ì • ê²°ê³¼
        """
        pricing = self._get_pricing(llm_provider, llm_model)

        return CostEstimateResponse(
            document_id=document_id,
            total_chunks=0,
            estimated_input_tokens=0,
            estimated_output_tokens=0,
            estimated_total_tokens=0,
            cost_breakdown=CostBreakdown(
                input_cost_usd=0.0,
                output_cost_usd=0.0,
                total_cost_usd=0.0
            ),
            cost_per_chunk_usd=0.0,
            model_pricing=ModelPricing(
                provider=llm_provider,
                model=llm_model,
                input_price_per_1k=pricing["input"],
                output_price_per_1k=pricing["output"]
            ),
            warnings=[
                "âš ï¸ ì´ ë¬¸ì„œì—ëŠ” ë¶„ì„ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•˜ì„¸ìš”."
            ]
        )
